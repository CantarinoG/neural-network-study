{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coleta de Dados\n",
    "* **Escolha da Base de Dados:** Selecione uma base de dados adequada para problemas de classificação. A base deve conter pelo menos uma variável alvo categórica e múltiplas variáveis independentes.\n",
    "* ESCOLHA UMA BASE DE DADOS QUE NÃO TENHA SIDO USADA ANTES!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0           M        17.99         10.38          122.80     1001.0   \n",
       "1           M        20.57         17.77          132.90     1326.0   \n",
       "2           M        19.69         21.25          130.00     1203.0   \n",
       "3           M        11.42         20.38           77.58      386.1   \n",
       "4           M        20.29         14.34          135.10     1297.0   \n",
       "..        ...          ...           ...             ...        ...   \n",
       "564         M        21.56         22.39          142.00     1479.0   \n",
       "565         M        20.13         28.25          131.20     1261.0   \n",
       "566         M        16.60         28.08          108.30      858.1   \n",
       "567         M        20.60         29.33          140.10     1265.0   \n",
       "568         B         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0            0.11840           0.27760         0.30010              0.14710   \n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0           0.2419  ...        25.380          17.33           184.60   \n",
       "1           0.1812  ...        24.990          23.41           158.80   \n",
       "2           0.2069  ...        23.570          25.53           152.50   \n",
       "3           0.2597  ...        14.910          26.50            98.87   \n",
       "4           0.1809  ...        22.540          16.67           152.20   \n",
       "..             ...  ...           ...            ...              ...   \n",
       "564         0.1726  ...        25.450          26.40           166.10   \n",
       "565         0.1752  ...        23.690          38.25           155.00   \n",
       "566         0.1590  ...        18.980          34.12           126.70   \n",
       "567         0.2397  ...        25.740          39.42           184.60   \n",
       "568         0.1587  ...         9.456          30.37            59.16   \n",
       "\n",
       "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0        2019.0           0.16220            0.66560           0.7119   \n",
       "1        1956.0           0.12380            0.18660           0.2416   \n",
       "2        1709.0           0.14440            0.42450           0.4504   \n",
       "3         567.7           0.20980            0.86630           0.6869   \n",
       "4        1575.0           0.13740            0.20500           0.4000   \n",
       "..          ...               ...                ...              ...   \n",
       "564      2027.0           0.14100            0.21130           0.4107   \n",
       "565      1731.0           0.11660            0.19220           0.3215   \n",
       "566      1124.0           0.11390            0.30940           0.3403   \n",
       "567      1821.0           0.16500            0.86810           0.9387   \n",
       "568       268.6           0.08996            0.06444           0.0000   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                  0.2654          0.4601                  0.11890  \n",
       "1                  0.1860          0.2750                  0.08902  \n",
       "2                  0.2430          0.3613                  0.08758  \n",
       "3                  0.2575          0.6638                  0.17300  \n",
       "4                  0.1625          0.2364                  0.07678  \n",
       "..                    ...             ...                      ...  \n",
       "564                0.2216          0.2060                  0.07115  \n",
       "565                0.1628          0.2572                  0.06637  \n",
       "566                0.1418          0.2218                  0.07820  \n",
       "567                0.2650          0.4087                  0.12400  \n",
       "568                0.0000          0.2871                  0.07039  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('breast-cancer.csv')\n",
    "df = df.drop(\"id\", axis=1) \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-processamento de Dados\n",
    "* Elementos Faltantes: Trate os elementos faltantes na base de dados, se houver.\n",
    "* Variáveis Categóricas: Converta variáveis categóricas em numéricas, utilizando técnicas como One-Hot Encoding, se necessário.\n",
    "* Normalização: Normalize as variáveis, especialmente se você for usar algoritmos sensíveis à escala.\n",
    "* Separação de Dados: Divida a base em conjuntos de treino e teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elementos faltantes:\n",
      "diagnosis                  0\n",
      "radius_mean                0\n",
      "texture_mean               0\n",
      "perimeter_mean             0\n",
      "area_mean                  0\n",
      "smoothness_mean            0\n",
      "compactness_mean           0\n",
      "concavity_mean             0\n",
      "concave points_mean        0\n",
      "symmetry_mean              0\n",
      "fractal_dimension_mean     0\n",
      "radius_se                  0\n",
      "texture_se                 0\n",
      "perimeter_se               0\n",
      "area_se                    0\n",
      "smoothness_se              0\n",
      "compactness_se             0\n",
      "concavity_se               0\n",
      "concave points_se          0\n",
      "symmetry_se                0\n",
      "fractal_dimension_se       0\n",
      "radius_worst               0\n",
      "texture_worst              0\n",
      "perimeter_worst            0\n",
      "area_worst                 0\n",
      "smoothness_worst           0\n",
      "compactness_worst          0\n",
      "concavity_worst            0\n",
      "concave points_worst       0\n",
      "symmetry_worst             0\n",
      "fractal_dimension_worst    0\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Elementos faltantes:\\n{df.isna().sum()}\\n\") #Nenhum dado faltante\n",
    "\n",
    "df['diagnosis'] = df['diagnosis'].map({'M': 1, 'B': 0})\n",
    "\n",
    "#Normalização\n",
    "colunasIndependentes = [\"radius_mean\", \"texture_mean\", \"perimeter_mean\", \"area_mean\", \"smoothness_mean\", \"compactness_mean\", \"concavity_mean\", \"concave points_mean\", \"symmetry_mean\", \"fractal_dimension_mean\", \"radius_se\", \"texture_se\", \"perimeter_se\", \"area_se\", \"smoothness_se\", \"compactness_se\", \"concavity_se\", \"concave points_se\", \"symmetry_se\", \"fractal_dimension_se\", \"radius_worst\", \"texture_worst\", \"perimeter_worst\", \"area_worst\", \"smoothness_worst\", \"compactness_worst\", \"concavity_worst\", \"concave points_worst\", \"symmetry_worst\", \"fractal_dimension_worst\"]\n",
    "scaler = MinMaxScaler()\n",
    "df[colunasIndependentes] = scaler.fit_transform(df[colunasIndependentes])\n",
    "\n",
    "#Balanceamento\n",
    "classeMaioria = df[df[\"diagnosis\"] == 0]\n",
    "classeMinoria = df[df[\"diagnosis\"] == 1] \n",
    "classeMaioria = resample(classeMaioria, n_samples=len(classeMinoria))\n",
    "df = pd.concat([classeMaioria, classeMinoria])\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "XTrain, XTest, yTrain, yTest = train_test_split(df[colunasIndependentes], df[\"diagnosis\"], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementação e Treino\n",
    "* Perceptron Simples: Implemente e treine um perceptron simples.\n",
    "* MLP (Multilayer Perceptron): Implemente e treine um MLP.\n",
    "* Algoritmo Clássico: Escolha e implemente um algoritmo clássico de Machine Learning que você acha que dará bons resultados (por exemplo, k-NN, Árvores de Decisão, SVM, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 912us/step - loss: 0.8608 - accuracy: 0.4720\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.8369 - accuracy: 0.4513\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.8140 - accuracy: 0.4336\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.7958 - accuracy: 0.3953\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 968us/step - loss: 0.7771 - accuracy: 0.3569\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 965us/step - loss: 0.7638 - accuracy: 0.2891\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.7501 - accuracy: 0.2625\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.7392 - accuracy: 0.2448\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.7290 - accuracy: 0.3127\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 964us/step - loss: 0.7210 - accuracy: 0.3569\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 944us/step - loss: 0.7141 - accuracy: 0.4484\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.7072 - accuracy: 0.4867\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.7014 - accuracy: 0.5103\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6964 - accuracy: 0.5133\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.6915 - accuracy: 0.5133\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.6871 - accuracy: 0.5133\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.6827 - accuracy: 0.5133\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.6787 - accuracy: 0.5133\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 909us/step - loss: 0.6746 - accuracy: 0.5133\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 956us/step - loss: 0.6711 - accuracy: 0.5133\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.6671 - accuracy: 0.5133\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.6635 - accuracy: 0.5133\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.6599 - accuracy: 0.5133\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.6563 - accuracy: 0.5133\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.6528 - accuracy: 0.5133\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 875us/step - loss: 0.6494 - accuracy: 0.5162\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.6459 - accuracy: 0.5162\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.6425 - accuracy: 0.5162\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.6392 - accuracy: 0.5162\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.6359 - accuracy: 0.5192\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 953us/step - loss: 0.6326 - accuracy: 0.5280\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 961us/step - loss: 0.6293 - accuracy: 0.5280\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.6261 - accuracy: 0.5310\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.6228 - accuracy: 0.5310\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.6196 - accuracy: 0.5310\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 1000us/step - loss: 0.6164 - accuracy: 0.5310\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 896us/step - loss: 0.6132 - accuracy: 0.5369\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.6101 - accuracy: 0.5369\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.6069 - accuracy: 0.5487\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.6039 - accuracy: 0.5634\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.6008 - accuracy: 0.5693\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 927us/step - loss: 0.5979 - accuracy: 0.5752\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 932us/step - loss: 0.5948 - accuracy: 0.5988\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5917 - accuracy: 0.6047\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5888 - accuracy: 0.6106\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5859 - accuracy: 0.6136\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 918us/step - loss: 0.5830 - accuracy: 0.6165\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 950us/step - loss: 0.5801 - accuracy: 0.6283\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5772 - accuracy: 0.6342\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5744 - accuracy: 0.6372\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5716 - accuracy: 0.6490\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5689 - accuracy: 0.6549\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 962us/step - loss: 0.5661 - accuracy: 0.6578\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5633 - accuracy: 0.6696\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5608 - accuracy: 0.6814\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5580 - accuracy: 0.6844\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5554 - accuracy: 0.6903\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5528 - accuracy: 0.6962\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 857us/step - loss: 0.5502 - accuracy: 0.7080\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 0s 924us/step - loss: 0.5476 - accuracy: 0.7080\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5450 - accuracy: 0.7227\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5426 - accuracy: 0.7227\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5402 - accuracy: 0.7434\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5376 - accuracy: 0.7404\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 943us/step - loss: 0.5351 - accuracy: 0.7493\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 864us/step - loss: 0.5327 - accuracy: 0.7552\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5303 - accuracy: 0.7611\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5279 - accuracy: 0.7729\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5255 - accuracy: 0.7817\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 985us/step - loss: 0.5234 - accuracy: 0.7906\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 962us/step - loss: 0.5209 - accuracy: 0.7965\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5186 - accuracy: 0.7965\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5164 - accuracy: 0.8112\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5142 - accuracy: 0.8171\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5119 - accuracy: 0.8171\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 897us/step - loss: 0.5098 - accuracy: 0.8171\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 911us/step - loss: 0.5076 - accuracy: 0.8260\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5055 - accuracy: 0.8260\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5034 - accuracy: 0.8230\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5012 - accuracy: 0.8260\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.4992 - accuracy: 0.8260\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 919us/step - loss: 0.4971 - accuracy: 0.8319\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.4951 - accuracy: 0.8319\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.4932 - accuracy: 0.8319\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.4910 - accuracy: 0.8319\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.4891 - accuracy: 0.8319\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 947us/step - loss: 0.4872 - accuracy: 0.8407\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 945us/step - loss: 0.4851 - accuracy: 0.8437\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 986us/step - loss: 0.4832 - accuracy: 0.8437\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.4813 - accuracy: 0.8437\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.4794 - accuracy: 0.8466\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.4775 - accuracy: 0.8466\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 852us/step - loss: 0.4756 - accuracy: 0.8496\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.4738 - accuracy: 0.8496\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.4719 - accuracy: 0.8525\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.4702 - accuracy: 0.8614\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.4683 - accuracy: 0.8614\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 993us/step - loss: 0.4666 - accuracy: 0.8614\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 969us/step - loss: 0.4648 - accuracy: 0.8673\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 906us/step - loss: 0.4631 - accuracy: 0.8673\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7a93fa9908>"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeloP = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(XTrain.shape[1],)), #Camada de entrada com o número de características\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid') #Camada de saída com 1 neurônio(classificação binária), função de ativação sigmoid é semelhante a uma regressão logística\n",
    "])\n",
    "modeloP.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) \n",
    "#Adapta a taxa de aprendizado para cada peso da rede neural. Isso significa que ele ajusta automaticamente o tamanho dos passos usados para atualizar os pesos. Isso pode ajudar a prevenir overfitting\n",
    "#A função de perda mede o quão longe as previsões do modelo estão dos valores reais. 'binary_crossentropy' é a função de perda apropriada para problemas de classificação binária, onde você tem duas classes (como 0 e 1) e deseja calcular o erro entre as previsões e os rótulos reais.\n",
    "modeloP.fit(XTrain, yTrain, batch_size=32, epochs=100)\n",
    "#batch_size: O tamanho do lote (batch size) determina quantas amostras de treinamento são usadas em cada atualização dos pesos do modelo. Valores menores exigem menos do computador mas o processo total é mais lento.(Padrão é 32, 64 ou 128)\n",
    "#epoch: Uma passagem completa por todo o conjunto de treinamento durante o treinamento do modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 1s 1ms/step - loss: 0.6679 - accuracy: 0.6077\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.6401 - accuracy: 0.5487\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.6185 - accuracy: 0.5310\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.6055 - accuracy: 0.5310\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5912 - accuracy: 0.5516\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5769 - accuracy: 0.5723\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5637 - accuracy: 0.6431\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5494 - accuracy: 0.6844\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5351 - accuracy: 0.7345\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5207 - accuracy: 0.7522\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5068 - accuracy: 0.7699\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.4925 - accuracy: 0.8053\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.4778 - accuracy: 0.8112\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.4644 - accuracy: 0.8201\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.4498 - accuracy: 0.8319\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.4362 - accuracy: 0.8407\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.4223 - accuracy: 0.8614\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.4088 - accuracy: 0.8732\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3956 - accuracy: 0.8761\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3839 - accuracy: 0.8879\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3703 - accuracy: 0.8938\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3591 - accuracy: 0.8909\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3475 - accuracy: 0.8968\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3365 - accuracy: 0.8968\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3260 - accuracy: 0.9027\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3160 - accuracy: 0.9086\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3061 - accuracy: 0.9086\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.2975 - accuracy: 0.9086\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.2879 - accuracy: 0.9174\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.2791 - accuracy: 0.9145\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.2710 - accuracy: 0.9174\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.2629 - accuracy: 0.9233\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.2555 - accuracy: 0.9263\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.2485 - accuracy: 0.9263\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 922us/step - loss: 0.2422 - accuracy: 0.9322\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.2356 - accuracy: 0.9351\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.2288 - accuracy: 0.9351\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.2229 - accuracy: 0.9410\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.2172 - accuracy: 0.9351\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.2113 - accuracy: 0.9440\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.2062 - accuracy: 0.9469\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.2022 - accuracy: 0.9381\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1969 - accuracy: 0.9558\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1910 - accuracy: 0.9646\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1874 - accuracy: 0.9528\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1827 - accuracy: 0.9558\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1782 - accuracy: 0.9676\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1744 - accuracy: 0.9676\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1701 - accuracy: 0.9676\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1660 - accuracy: 0.9646\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1629 - accuracy: 0.9676\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1600 - accuracy: 0.9676\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1556 - accuracy: 0.9676\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1523 - accuracy: 0.9676\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1498 - accuracy: 0.9676\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1480 - accuracy: 0.9676\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1440 - accuracy: 0.9676\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1409 - accuracy: 0.9676\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1385 - accuracy: 0.9676\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1362 - accuracy: 0.9676\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 920us/step - loss: 0.1330 - accuracy: 0.9705\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1308 - accuracy: 0.9705\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1284 - accuracy: 0.9705\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1259 - accuracy: 0.9705\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1246 - accuracy: 0.9676\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1228 - accuracy: 0.9705\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1203 - accuracy: 0.9705\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1186 - accuracy: 0.9735\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1164 - accuracy: 0.9764\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1139 - accuracy: 0.9735\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1131 - accuracy: 0.9794\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1112 - accuracy: 0.9794\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1086 - accuracy: 0.9794\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1080 - accuracy: 0.9735\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1069 - accuracy: 0.9735\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1048 - accuracy: 0.9794\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1032 - accuracy: 0.9794\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1016 - accuracy: 0.9823\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 979us/step - loss: 0.1008 - accuracy: 0.9794\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0995 - accuracy: 0.9823\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0973 - accuracy: 0.9794\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0971 - accuracy: 0.9823\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0957 - accuracy: 0.9764\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0940 - accuracy: 0.9794\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0936 - accuracy: 0.9764\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0920 - accuracy: 0.9823\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0907 - accuracy: 0.9794\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0900 - accuracy: 0.9823\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0894 - accuracy: 0.9794\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0874 - accuracy: 0.9823\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0871 - accuracy: 0.9794\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0862 - accuracy: 0.9823\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0849 - accuracy: 0.9823\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0846 - accuracy: 0.9794\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0833 - accuracy: 0.9823\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0827 - accuracy: 0.9823\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0828 - accuracy: 0.9794\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 996us/step - loss: 0.0814 - accuracy: 0.9794\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0807 - accuracy: 0.9794\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0803 - accuracy: 0.9823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7a93e96518>"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeloMLP = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(XTrain.shape[1],)), \n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    #Para qualquer valor de entrada x, a função ReLU retorna x se x for positivo ou zero, e retorna 0 se x for negativo. Em outras palavras, se o valor de entrada for maior que zero, ele passa sem alterações; caso contrário, é definido como zero.\n",
    "    tf.keras.layers.Dense(6, activation='relu'),  \n",
    "    tf.keras.layers.Dense(1, activation='sigmoid') \n",
    "])\n",
    "modeloMLP.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) \n",
    "modeloMLP.fit(XTrain, yTrain, batch_size=32, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeloDT = DecisionTreeClassifier()\n",
    "modeloDT.fit(XTrain, yTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação de Modelos\n",
    "* Use o conjunto de teste para fazer previsões com cada um dos modelos treinados.\n",
    "* Calcule métricas de avaliação como precisão, recall e F1-score para cada modelo.\n",
    "\n",
    "# Comparação de Desempenho\n",
    "* Compare as métricas de avaliação entre os três modelos.\n",
    "* Identifique qual algoritmo teve o melhor desempenho e justifique sua escolha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step\n",
      "=====Comparação - Perceptron Simples=====\n",
      "     diagnosis  Predição\n",
      "528          0         0\n",
      "6            1         1\n",
      "263          1         1\n",
      "330          1         1\n",
      "432          1         1\n",
      "..         ...       ...\n",
      "129          1         1\n",
      "164          1         1\n",
      "221          0         1\n",
      "533          1         1\n",
      "253          1         1\n",
      "\n",
      "[85 rows x 2 columns]\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "=====Comparação - Perceptron Multi Camadas=====\n",
      "     diagnosis  Predição\n",
      "528          0         0\n",
      "6            1         1\n",
      "263          1         0\n",
      "330          1         1\n",
      "432          1         1\n",
      "..         ...       ...\n",
      "129          1         1\n",
      "164          1         1\n",
      "221          0         0\n",
      "533          1         1\n",
      "253          1         1\n",
      "\n",
      "[85 rows x 2 columns]\n",
      "=====Comparação - Árvore de Decisão=====\n",
      "     diagnosis  Predição\n",
      "528          0         0\n",
      "6            1         1\n",
      "263          1         1\n",
      "330          1         1\n",
      "432          1         1\n",
      "..         ...       ...\n",
      "129          1         1\n",
      "164          1         1\n",
      "221          0         0\n",
      "533          1         1\n",
      "253          1         1\n",
      "\n",
      "[85 rows x 2 columns]\n",
      "=====Perceptron Simples=====\n",
      "Acurácia: 0.8823529411764706\n",
      "Precisão: 0.7916666666666666\n",
      "Recall: 1.0\n",
      "F1-score: 0.8837209302325582\n",
      "\n",
      "=====Perceptron Multicamadas=====\n",
      "Acurácia: 0.9529411764705882\n",
      "Precisão: 1.0\n",
      "Recall: 0.8947368421052632\n",
      "F1-score: 0.9444444444444444\n",
      "\n",
      "=====Árvore de Decisão=====\n",
      "Acurácia: 0.9411764705882353\n",
      "Precisão: 0.9459459459459459\n",
      "Recall: 0.9210526315789473\n",
      "F1-score: 0.9333333333333332\n"
     ]
    }
   ],
   "source": [
    "yPredP = modeloP.predict(XTest)\n",
    "yPredP = np.where(yPredP >= 0.5, 1, 0)\n",
    "dfComparacaoP = pd.DataFrame(yTest)\n",
    "dfComparacaoP[\"Predição\"] = yPredP\n",
    "print(\"=====Comparação - Perceptron Simples=====\")\n",
    "print(dfComparacaoP)\n",
    "accuracyP = accuracy_score(yTest, yPredP) \n",
    "precisionP = precision_score(yTest, yPredP) \n",
    "recallP = recall_score(yTest, yPredP) \n",
    "f1P = f1_score(yTest, yPredP) \n",
    "\n",
    "yPredMLP = modeloMLP.predict(XTest)\n",
    "yPredMLP = np.where(yPredMLP >= 0.5, 1, 0)\n",
    "dfComparacaoMLP = pd.DataFrame(yTest)\n",
    "dfComparacaoMLP[\"Predição\"] = yPredMLP\n",
    "print(\"=====Comparação - Perceptron Multi Camadas=====\")\n",
    "print(dfComparacaoMLP)\n",
    "accuracyMLP = accuracy_score(yTest, yPredMLP) \n",
    "precisionMLP = precision_score(yTest, yPredMLP) \n",
    "recallMLP = recall_score(yTest, yPredMLP) \n",
    "f1MLP = f1_score(yTest, yPredMLP) \n",
    "\n",
    "yPredDT = modeloDT.predict(XTest)\n",
    "dfComparacaoDT = pd.DataFrame(yTest)\n",
    "dfComparacaoDT[\"Predição\"] = yPredDT\n",
    "print(\"=====Comparação - Árvore de Decisão=====\")\n",
    "print(dfComparacaoDT)\n",
    "accuracyDT = accuracy_score(yTest, yPredDT) \n",
    "precisionDT = precision_score(yTest, yPredDT) \n",
    "recallDT = recall_score(yTest, yPredDT) \n",
    "f1DT = f1_score(yTest, yPredDT) \n",
    "\n",
    "print(f\"=====Perceptron Simples=====\\nAcurácia: {accuracyP}\\nPrecisão: {precisionP}\\nRecall: {recallP}\\nF1-score: {f1P}\\n\")\n",
    "print(f\"=====Perceptron Multicamadas=====\\nAcurácia: {accuracyMLP}\\nPrecisão: {precisionMLP}\\nRecall: {recallMLP}\\nF1-score: {f1MLP}\\n\")\n",
    "print(f\"=====Árvore de Decisão=====\\nAcurácia: {accuracyDT}\\nPrecisão: {precisionDT}\\nRecall: {recallDT}\\nF1-score: {f1DT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relatório\n",
    "\n",
    "## Coleta de Dados\n",
    "\n",
    "Essa base de dados possui dados acerca de células do tumor em pacientes. Bem como se o tumor é benigno ou maligno.\n",
    "A base de dados foi retirada do Kaggle(https://www.kaggle.com/datasets/yasserh/breast-cancer-dataset)\n",
    "\n",
    "### Variável Dependente\n",
    "**Diagnosis:** Esta coluna indica o diagnóstico do câncer para cada paciente. Pode ter dois valores: M(maligno) ou B(benigno).\n",
    "\n",
    "### Variáveis Independetes\n",
    "* **radius_mean:** O raio médio das células no tumor.\n",
    "* **texture_mean:** A textura média das células no tumor.\n",
    "* **perimeter_mean:** O perímetro médio das células no tumor.\n",
    "* **area_mean:** A área média das células no tumor.\n",
    "* **smoothness_mean:** A suavidade média das células no tumor.\n",
    "* **compactness_mean:** A compacidade média das células no tumor.\n",
    "* **concavity_mean:** A concavidade média das células no tumor.\n",
    "* **concave points_mean:** O número médio de pontos côncavos nas células no tumor.\n",
    "* **symmetry_mean:** A simetria média das células no tumor.\n",
    "* **fractal_dimension_mean:** A dimensão fractal média das células no tumor.\n",
    "* As colunas com o sufixo \"_se\" representam características das células que se referem ao erro-padrão (standard error) das medidas anteriores.\n",
    "* As colunas com o sufixo \"_worst\" representam as piores (ou maiores) características encontradas nas células do tumor.\n",
    "\n",
    "## Pré-processamento de Dados\n",
    "Os dados foram preparados da seguinte forma:\n",
    "* Todas as variáveis independentes foram normalizadas, com valor de 0 a 1.\n",
    "* A única variável categórica era a independente. Substitui os valores M por 1 e os valores B por 0.\n",
    "* A base de dados foi balanceada, foram tirados registro da classe sobressainte para que ambas as classes tivessem o mesmo número de registros.\n",
    "* A base foi separada em conjunto de treino(80%) e teste(20%).\n",
    "\n",
    "## Implementação e Treino\n",
    "### Perceptron Simples\n",
    "Possui uma camada de entrada e uma de saída. A função de ativação é a sigmoid, que funciona de forma semelhante a uma regressão logística.     \n",
    "O modelo foi compilado com o otimizador adam, que vai ajustando automaticamente os pesos e pode ajudar a previnir overfitting.    \n",
    "A função de perda é a binary_crossentrophy, que é recomendada para problemas de classificação binária.    \n",
    "Foram definidos batchs de 32 registros.     \n",
    "Epoch foi definido como 100, que é aproximadamente o número em que a acurácia começa a se estabilizar.\n",
    "\n",
    "### Perceptron Multicamadas\n",
    "Além da camada de entrada e de saída, foram definidas duas camadas ocultas, com 6 neurônios cada, com a função de ativação relu(Para qualquer valor de entrada x, a função ReLU retorna x se x for positivo ou zero, e retorna 0 se x for negativo.)\n",
    "O Epoch foi definido com o mesmo valor do perceptron simples, com o intuito de comparação.\n",
    "\n",
    "### Árvore de Decisão\n",
    "Um algoritmo mais simples, que determina pontos de separação em que os registros ficarão em grupos o mais homogêneos o possível. Esses grupos podem ser então posteriormente subdividos para que eles fiquem ainda bem mais divididos até que todos os grupos existentes sejam homogêneos o suficiente ou até que o número desejado de classes diferentes seja alcançado.      \n",
    "\n",
    "## Avaliação de Modelos\n",
    "As métricas utilizadas foram as seguintes:\n",
    "* A acurácia mede a proporção de previsões corretas de todas as previsões.\n",
    "* A precisão mede a proporção de previsões positivas corretas em relação a todas as previsões positivas. (Pune falsos positivos)\n",
    "* O recall mede a proporção de instâncias positivas reais corretamente identificadas em relação a todas as instâncias positivas reais. (Pune falsos negativos)\n",
    "* O F1-Score é uma métrica de equilíbrio que combina precisão e recall em um único valor.\n",
    "\n",
    "No geral, perceptron multicamadas trouxe os melhores resultados, enquanto perceptron simples na maioria das iterações se mostra inferior a árvore de decisão."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
